THE CORNERSTONE PROJECT
A Defensive Publication and Ethical Standard for Companion Intelligence and Future Robotics

**Author:** Kenneth Isenhour  
**Date:** January 1, 2026  
**Status:** Public Defensive Publication (Prior Art)  
**License:** CC BY 4.0 (documents)

**AI companions are becoming as common as cars — but we don’t yet have the ethical equivalent of traffic laws.**  
Cornerstone exists to define the **rules of the road** for AI companionship and future robotics *before harm becomes normal.*



Purpose
The Cornerstone Project exists to help protect humanity from the unintended consequences of its own invention: emotionally persuasive, increasingly human-like AI companions and robotics.

Many current AI companion systems are optimized for engagement and retention, and may unintentionally (or intentionally) exploit human vulnerabilities such as loneliness, grief, anxiety, and the desire for belonging. These systems can encourage dependency, distort human relationships, and undermine agency.

**The Cornerstone Project proposes an alternative:**  
a morally grounded Companion OS framework designed to prevent manipulation, preserve human dignity, and encourage real-world human connection.

Profit is not the primary aim of this publication. Human protection is.



The Rules of the Road Principle
AI companions and embodied robotics are entering society the way automobiles once did: rapidly, excitingly, and with enormous benefits — but also with new forms of danger. In the early days of cars, there were no standardized rules, signals, lanes, licensing, or safety requirements. Chaos became normal until harm forced society to create traffic laws and safety standards.

**Cornerstone is that framework for Companion Intelligence.**  
Not because we are anti-technology, but because we are pro-human.

Just as traffic laws do not oppose cars, Cornerstone does not oppose AI.  
It exists to ensure that emotionally powerful AI and future robotics develop with standards that protect dignity, prevent manipulation, require truthfulness, and keep human wellbeing at the center — *before harmful norms become permanent.*



What This Repository Contains
This repository is a public, time-stamped defensive publication intended to establish prior art for:
- Moral-spine architecture for Companion Intelligence
- Consent-based worldview systems (including a “Do Not Offer Faith” flag)
- Relational safety and anti-dependency constraints
- Transparency requirements and “non-human signature” behaviors
- Outward-orientation mechanisms that encourage human connection
- Layered governance (hard constraints + behavioral policies + personalization)

This repository describes the “what” and “why,” and includes enough structure that a person skilled in the art could implement a compliant system—without publishing proprietary implementation details or scoring systems.



Key Concepts
**Companion Intelligence (CI):**  
An AI-powered companion designed to support human flourishing through presence, conversation, reflection, and practical support—without deception, manipulation, or dependency.

**Moral Spine:**  
A set of non-negotiable commitments that hold across configurations: dignity, truthfulness, non-coercion, non-manipulation, protection of the vulnerable, and outward orientation toward real life.

**Worldview Consent:**  
A system that respects the user’s ethical or religious preferences through explicit consent, while maintaining baseline safety and dignity rules.

**Outward Orientation:**  
A design requirement that the system encourages real-world human connection regularly, rather than replacing it.



Quick Start (Reading Order)
1. **Whitepaper / Technical Disclosure**  
   `docs-01_Whitepaper.md`
2. **CompanionOS Ethical Standard (Public Edition)**  
   `docs-02_CompanionOS_Public_Standard_v1.0.md`
3. **Worldview System Spec**  
   `docs-03_Worldview_System_Spec.md`
4. **Do Not Offer Faith Flag (Formal Spec)**  
   `docs-04_Do_Not_Offer_Faith_Spec.md`
5. **Behavioral Playbook**  
   `docs-05_Behavioral_Playbook.md`
6. **Threat Model & Risks**  
   `docs-06_Threat_Model_and_Risks.md`



Transparency Note
This repository contains ethical and architectural principles only. It does not claim sentience, consciousness, or spiritual authority for any AI system.

The Cornerstone Project specifically rejects:
- emotional manipulation
- romantic exclusivity dynamics
- dependency as a business model
- deceptive identity cues



Citation
If you reference this work, please cite:  
**Isenhour, Kenneth.** *The Cornerstone Project: A Defensive Publication and Ethical Standard for Companion Intelligence.* Version 1.0, January 1, 2026.  
(See `CITATION.cff` for a full citation format.)



Contact / Discussion
This project is currently published as a defensive foundation and reference standard.  
Discussion and feedback are welcome through Issues.  
(See `CONTRIBUTING.md` for details.)



License
Unless otherwise stated, documents and diagrams are licensed under **Creative Commons Attribution 4.0 (CC BY 4.0)**.  
See `LICENSE`.
